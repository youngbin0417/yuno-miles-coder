# ğŸŒ©ï¸ Cloud LLM (neutral explanation)
# - OpenAI style: CLOUD_BASE_URL=https://api.openai.com/v1, model=gpt-4o-mini
# - Gemini: CLOUD_BASE_URL=https://generativelanguage.googleapis.com/v1beta, CLOUD_MODEL=gemini-1.5-pro
# - OpenRouter: CLOUD_BASE_URL=https://openrouter.ai/api/v1, CLOUD_MODEL=openai/gpt-oss-120b:free
CLOUD_BASE_URL=https://generativelanguage.googleapis.com/v1beta
CLOUD_MODEL=gemini-1.5-pro
CLOUD_API_KEY=your-gemini-key

# ğŸ–¥ï¸ Local LLM (style transform, â‰¤7B recommended)
# Ollama example (qwen2.5:3b): base=http://127.0.0.1:11434, model=qwen2.5:3b
# OpenAI-compatible (LM Studio/vLLM) servers should expose a /v1 endpoint.
LOCAL_BASE_URL=http://127.0.0.1:11434
LOCAL_MODEL=qwen2.5:3b
LOCAL_API_KEY=local

# ğŸš¦ Mode selection
# hybrid = Plan B (cloud explanation + local meme-style transform)
# local  = use only local model
# cloud  = use only cloud model
MODE=hybrid

# ğŸ”Š Options
BLEEP=1              # 1 = censor profanity, 0 = raw chaos
PERSONA=chaotic_microblog

# ì„ íƒ: í…ìŠ¤íŠ¸ ì—†ëŠ” ì¸ë±ìŠ¤ íŒ© ê²½ë¡œ
YMC_PACK=/absolute/path/to/my-pack.ymcpack
