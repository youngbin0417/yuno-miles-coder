# 🌩️ Cloud LLM (neutral explanation)
# - OpenAI style: CLOUD_BASE_URL=https://api.openai.com/v1, model=gpt-4o-mini
# - Gemini:      CLOUD_BASE_URL=https://generativelanguage.googleapis.com/v1beta
CLOUD_BASE_URL=https://generativelanguage.googleapis.com/v1beta
CLOUD_MODEL=gemini-1.5-pro
CLOUD_API_KEY=your-gemini-key

# 🖥️ Local LLM (style transform, ≤7B recommended)
# Ollama example (qwen2.5:3b): base=http://127.0.0.1:11434, model=qwen2.5:3b
# OpenAI-compatible (LM Studio/vLLM) servers should expose a /v1 endpoint.
LOCAL_BASE_URL=http://127.0.0.1:11434
LOCAL_MODEL=qwen2.5:3b
LOCAL_API_KEY=local

# 🚦 Mode selection
# hybrid = Plan B (cloud explanation + local meme-style transform)
# local  = use only local model
# cloud  = use only cloud model
MODE=hybrid

# 🔊 Options
BLEEP=1              # 1 = censor profanity, 0 = raw chaos
PERSONA=chaotic_microblog

# 선택: 텍스트 없는 인덱스 팩 경로
YMC_PACK=/absolute/path/to/my-pack.ymcpack
